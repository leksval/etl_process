{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = data.load_data(num_words=10000)\n",
    "# only words that appera over 10000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_index(data):\n",
    "    word_index = data.get_word_index()\n",
    "    word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "    word_index[\"<PAD>\"] = 0\n",
    "    word_index[\"<START>\"] = 1\n",
    "    word_index[\"<UNK>\"] = 2\n",
    "    word_index[\"<INUSED>\"] = 3\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = make_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key,value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value = word_index[\"<PAD>\"], padding=\"post\", maxlen = 250)\n",
    "test_data  = keras.preprocessing.sequence.pad_sequences(test_data, value = word_index[\"<PAD>\"], padding=\"post\", maxlen = 250)\n",
    "# data must have the same len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 250\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data[0]),len(test_data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(text):\n",
    "    return \" \".join([reverse_word_index.get(i, \"?\") for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Bartosz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# now defining model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(88000, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1,  activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          1408000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,408,289\n",
      "Trainable params: 1,408,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "x_val = train_data[:10000]\n",
    "x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "y_train = train_labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\Bartosz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/35\n",
      "15000/15000 [==============================] - 3s 205us/sample - loss: 0.6924 - acc: 0.5325 - val_loss: 0.6911 - val_acc: 0.5217\n",
      "Epoch 2/35\n",
      "15000/15000 [==============================] - 2s 120us/sample - loss: 0.6881 - acc: 0.6564 - val_loss: 0.6849 - val_acc: 0.7157\n",
      "Epoch 3/35\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.6782 - acc: 0.7374 - val_loss: 0.6721 - val_acc: 0.7510\n",
      "Epoch 4/35\n",
      "15000/15000 [==============================] - 2s 108us/sample - loss: 0.6598 - acc: 0.7541 - val_loss: 0.6501 - val_acc: 0.7542\n",
      "Epoch 5/35\n",
      "15000/15000 [==============================] - 2s 107us/sample - loss: 0.6303 - acc: 0.7849 - val_loss: 0.6180 - val_acc: 0.7773\n",
      "Epoch 6/35\n",
      "15000/15000 [==============================] - 2s 111us/sample - loss: 0.5904 - acc: 0.8047 - val_loss: 0.5784 - val_acc: 0.7963\n",
      "Epoch 7/35\n",
      "15000/15000 [==============================] - 2s 111us/sample - loss: 0.5438 - acc: 0.8203 - val_loss: 0.5340 - val_acc: 0.8112\n",
      "Epoch 8/35\n",
      "15000/15000 [==============================] - 2s 109us/sample - loss: 0.4957 - acc: 0.8391 - val_loss: 0.4918 - val_acc: 0.8257\n",
      "Epoch 9/35\n",
      "15000/15000 [==============================] - 2s 112us/sample - loss: 0.4501 - acc: 0.8529 - val_loss: 0.4530 - val_acc: 0.8383\n",
      "Epoch 10/35\n",
      "15000/15000 [==============================] - 2s 109us/sample - loss: 0.4094 - acc: 0.8667 - val_loss: 0.4204 - val_acc: 0.8450\n",
      "Epoch 11/35\n",
      "15000/15000 [==============================] - 2s 107us/sample - loss: 0.3744 - acc: 0.8763 - val_loss: 0.3929 - val_acc: 0.8557\n",
      "Epoch 12/35\n",
      "15000/15000 [==============================] - 2s 107us/sample - loss: 0.3444 - acc: 0.8852 - val_loss: 0.3715 - val_acc: 0.8584\n",
      "Epoch 13/35\n",
      "15000/15000 [==============================] - 2s 109us/sample - loss: 0.3201 - acc: 0.8913 - val_loss: 0.3529 - val_acc: 0.8660\n",
      "Epoch 14/35\n",
      "15000/15000 [==============================] - 2s 108us/sample - loss: 0.2985 - acc: 0.8979 - val_loss: 0.3395 - val_acc: 0.8702\n",
      "Epoch 15/35\n",
      "15000/15000 [==============================] - 2s 111us/sample - loss: 0.2807 - acc: 0.9018 - val_loss: 0.3286 - val_acc: 0.8720\n",
      "Epoch 16/35\n",
      "15000/15000 [==============================] - 2s 110us/sample - loss: 0.2648 - acc: 0.9081 - val_loss: 0.3194 - val_acc: 0.8742\n",
      "Epoch 17/35\n",
      "15000/15000 [==============================] - 2s 109us/sample - loss: 0.2503 - acc: 0.9127 - val_loss: 0.3120 - val_acc: 0.8766\n",
      "Epoch 18/35\n",
      "15000/15000 [==============================] - 2s 109us/sample - loss: 0.2374 - acc: 0.9166 - val_loss: 0.3056 - val_acc: 0.8789\n",
      "Epoch 19/35\n",
      "15000/15000 [==============================] - 2s 110us/sample - loss: 0.2259 - acc: 0.9203 - val_loss: 0.3001 - val_acc: 0.8805\n",
      "Epoch 20/35\n",
      "15000/15000 [==============================] - 2s 110us/sample - loss: 0.2154 - acc: 0.9243 - val_loss: 0.2965 - val_acc: 0.8817\n",
      "Epoch 21/35\n",
      "15000/15000 [==============================] - 2s 109us/sample - loss: 0.2049 - acc: 0.9289 - val_loss: 0.2933 - val_acc: 0.8831\n",
      "Epoch 22/35\n",
      "15000/15000 [==============================] - 2s 109us/sample - loss: 0.1961 - acc: 0.9325 - val_loss: 0.2906 - val_acc: 0.8831\n",
      "Epoch 23/35\n",
      "15000/15000 [==============================] - 2s 108us/sample - loss: 0.1871 - acc: 0.9369 - val_loss: 0.2895 - val_acc: 0.8829\n",
      "Epoch 24/35\n",
      "15000/15000 [==============================] - 2s 110us/sample - loss: 0.1794 - acc: 0.9405 - val_loss: 0.2874 - val_acc: 0.8847\n",
      "Epoch 25/35\n",
      "15000/15000 [==============================] - 2s 113us/sample - loss: 0.1715 - acc: 0.9439 - val_loss: 0.2860 - val_acc: 0.8852\n",
      "Epoch 26/35\n",
      "15000/15000 [==============================] - 2s 111us/sample - loss: 0.1645 - acc: 0.9473 - val_loss: 0.2865 - val_acc: 0.8834\n",
      "Epoch 27/35\n",
      "15000/15000 [==============================] - 2s 106us/sample - loss: 0.1578 - acc: 0.9492 - val_loss: 0.2859 - val_acc: 0.8840\n",
      "Epoch 28/35\n",
      "15000/15000 [==============================] - 2s 107us/sample - loss: 0.1514 - acc: 0.9524 - val_loss: 0.2860 - val_acc: 0.8848\n",
      "Epoch 29/35\n",
      "15000/15000 [==============================] - 2s 106us/sample - loss: 0.1460 - acc: 0.9551 - val_loss: 0.2881 - val_acc: 0.8841\n",
      "Epoch 30/35\n",
      "15000/15000 [==============================] - 2s 107us/sample - loss: 0.1401 - acc: 0.9567 - val_loss: 0.2872 - val_acc: 0.8855\n",
      "Epoch 31/35\n",
      "15000/15000 [==============================] - 2s 107us/sample - loss: 0.1342 - acc: 0.9599 - val_loss: 0.2882 - val_acc: 0.8865\n",
      "Epoch 32/35\n",
      "15000/15000 [==============================] - 2s 106us/sample - loss: 0.1289 - acc: 0.9627 - val_loss: 0.2897 - val_acc: 0.8860\n",
      "Epoch 33/35\n",
      "15000/15000 [==============================] - 2s 108us/sample - loss: 0.1237 - acc: 0.9639 - val_loss: 0.2920 - val_acc: 0.8855\n",
      "Epoch 34/35\n",
      "15000/15000 [==============================] - 2s 108us/sample - loss: 0.1192 - acc: 0.9656 - val_loss: 0.2938 - val_acc: 0.8871\n",
      "Epoch 35/35\n",
      "15000/15000 [==============================] - 2s 110us/sample - loss: 0.1149 - acc: 0.9666 - val_loss: 0.2965 - val_acc: 0.8858\n"
     ]
    }
   ],
   "source": [
    "fitModel = model.fit(x_train, y_train, epochs = 35, batch_size = 512, validation_data = (x_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 2s 63us/sample - loss: 0.3152 - acc: 0.8737\n",
      "[0.3151730310535431, 0.87368]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data, test_labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      "<START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Prediction: [0.]\n",
      "Accual: 0\n",
      "[0.3151730310535431, 0.87368]\n"
     ]
    }
   ],
   "source": [
    "test_review = test_data[0]\n",
    "predict = model.predict([test_review])\n",
    "print(\"Review: \")\n",
    "print(decode_review(test_review))\n",
    "print(\"Prediction: \" + str(predict[0]))\n",
    "print(\"Accual: \" + str(test_labels[0]))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  review_encode(s):\n",
    "    encoded = [1]\n",
    "    for word in s:\n",
    "        if word.lower() in word_index:\n",
    "            encoded.append(word_index[word.lower()])\n",
    "        else:\n",
    "            encoded.append(2)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
